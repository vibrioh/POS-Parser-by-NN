UNI: jh3846
NAME: Jun Hu

Instruction: I use the old version of homework repository.

Part 1

Use following command:
python src/parser.py --vocabs model/vocabs --model model/model

(nlphw)  ~/local_projects/nlp_hw_dep  python src/parser.py --vocabs model/vocabs --model model/model
[dynet] random seed: 2569169847
[dynet] allocating memory: 512MB
[dynet] memory allocation done.
started epoch 1
current minibatch loss 2.33860492706 progress: 6.96 %
current minibatch loss 1.86187815666 progress: 13.91 %
current minibatch loss 1.6791049242 progress: 20.87 %
current minibatch loss 1.26847159863 progress: 27.82 %
current minibatch loss 1.12271273136 progress: 34.78 %
current minibatch loss 1.06385838985 progress: 41.74 %
current minibatch loss 0.935145735741 progress: 48.69 %
current minibatch loss 0.73326086998 progress: 55.65 %
current minibatch loss 0.692936778069 progress: 62.61 %
current minibatch loss 0.606771767139 progress: 69.56 %
current minibatch loss 0.518205821514 progress: 76.52 %
current minibatch loss 0.57952016592 progress: 83.47 %
current minibatch loss 0.455320268869 progress: 90.43 %
current minibatch loss 0.423405557871 progress: 97.39 %
started epoch 2
current minibatch loss 0.369573950768 progress: 4.87 %
current minibatch loss 0.377915114164 progress: 11.83 %
current minibatch loss 0.30652064085 progress: 18.78 %
current minibatch loss 0.316282868385 progress: 25.74 %
current minibatch loss 0.325290232897 progress: 32.69 %
current minibatch loss 0.297950863838 progress: 39.65 %
current minibatch loss 0.2890855968 progress: 46.61 %
current minibatch loss 0.340481519699 progress: 53.56 %
current minibatch loss 0.248985528946 progress: 60.52 %
current minibatch loss 0.283487826586 progress: 67.47 %
current minibatch loss 0.332979112864 progress: 74.43 %
current minibatch loss 0.249091595411 progress: 81.39 %
current minibatch loss 0.295750945807 progress: 88.34 %
current minibatch loss 0.241751223803 progress: 95.3 %
started epoch 3
current minibatch loss 0.269159317017 progress: 2.78 %
current minibatch loss 0.299786865711 progress: 9.74 %
current minibatch loss 0.209663406014 progress: 16.69 %
current minibatch loss 0.253117322922 progress: 23.65 %
current minibatch loss 0.17523573339 progress: 30.61 %
current minibatch loss 0.20959097147 progress: 37.56 %
current minibatch loss 0.224825367332 progress: 44.52 %
current minibatch loss 0.204502567649 progress: 51.48 %
current minibatch loss 0.211845368147 progress: 58.43 %
current minibatch loss 0.243757233024 progress: 65.39 %
current minibatch loss 0.184054359794 progress: 72.34 %
current minibatch loss 0.219360694289 progress: 79.3 %
current minibatch loss 0.227193504572 progress: 86.26 %
current minibatch loss 0.212466269732 progress: 93.21 %
started epoch 4
current minibatch loss 0.208717629313 progress: 0.7 %
current minibatch loss 0.209307849407 progress: 7.65 %
current minibatch loss 0.157659068704 progress: 14.61 %
current minibatch loss 0.13834849 progress: 21.56 %
current minibatch loss 0.185705602169 progress: 28.52 %
current minibatch loss 0.189190670848 progress: 35.48 %
current minibatch loss 0.187151610851 progress: 42.43 %
current minibatch loss 0.217627465725 progress: 49.39 %
current minibatch loss 0.178006514907 progress: 56.34 %
current minibatch loss 0.16458594799 progress: 63.3 %
current minibatch loss 0.184919953346 progress: 70.26 %
current minibatch loss 0.158946156502 progress: 77.21 %
current minibatch loss 0.193257302046 progress: 84.17 %
current minibatch loss 0.160877659917 progress: 91.13 %
current minibatch loss 0.134703725576 progress: 98.08 %
started epoch 5
current minibatch loss 0.146604984999 progress: 5.56 %
current minibatch loss 0.149867385626 progress: 12.52 %
current minibatch loss 0.120854794979 progress: 19.48 %
current minibatch loss 0.146512046456 progress: 26.43 %
current minibatch loss 0.140966370702 progress: 33.39 %
current minibatch loss 0.167688339949 progress: 40.35 %
current minibatch loss 0.137563705444 progress: 47.3 %
current minibatch loss 0.120833300054 progress: 54.26 %
current minibatch loss 0.140552818775 progress: 61.21 %
current minibatch loss 0.145722717047 progress: 68.17 %
current minibatch loss 0.141269534826 progress: 75.13 %
current minibatch loss 0.145700573921 progress: 82.08 %
current minibatch loss 0.125930204988 progress: 89.04 %
current minibatch loss 0.121768541634 progress: 95.99 %
started epoch 6
current minibatch loss 0.099287353456 progress: 3.48 %
current minibatch loss 0.107136294246 progress: 10.43 %
current minibatch loss 0.0953500643373 progress: 17.39 %
current minibatch loss 0.0915997326374 progress: 24.35 %
current minibatch loss 0.0843328312039 progress: 31.3 %
current minibatch loss 0.102401711047 progress: 38.26 %
current minibatch loss 0.13019014895 progress: 45.21 %
current minibatch loss 0.101859748363 progress: 52.17 %
current minibatch loss 0.0963231772184 progress: 59.13 %
current minibatch loss 0.129759758711 progress: 66.08 %
current minibatch loss 0.112133517861 progress: 73.04 %
current minibatch loss 0.105353154242 progress: 80.0 %
current minibatch loss 0.0866275578737 progress: 86.95 %
current minibatch loss 0.126431435347 progress: 93.91 %
started epoch 7
current minibatch loss 0.0879110619426 progress: 1.39 %
current minibatch loss 0.0942259579897 progress: 8.35 %
current minibatch loss 0.107482418418 progress: 15.3 %
current minibatch loss 0.0870024859905 progress: 22.26 %
current minibatch loss 0.0866082906723 progress: 29.22 %
current minibatch loss 0.0724126622081 progress: 36.17 %
current minibatch loss 0.0736460909247 progress: 43.13 %
current minibatch loss 0.108538024127 progress: 50.08 %
current minibatch loss 0.0835977643728 progress: 57.04 %
current minibatch loss 0.0946379229426 progress: 64.0 %
current minibatch loss 0.0657220259309 progress: 70.95 %
current minibatch loss 0.0787594169378 progress: 77.91 %
current minibatch loss 0.101714998484 progress: 84.86 %
current minibatch loss 0.119398400187 progress: 91.82 %
current minibatch loss 0.126107111573 progress: 98.78 %

Then execute:
python src/depModel.py trees/dev.conll outputs/dev_part1.conll

(nlphw)  ~/local_projects/nlp_hw_dep  python src/depModel.py trees/dev.conll outputs/dev_part1.conll
[dynet] random seed: 2505215724
[dynet] allocating memory: 512MB
[dynet] memory allocation done.
100...200...300...400...500...600...700...800...900...1000...1100...1200...1300...1400...1500...1600...1700...

Then evaluate the result:
python src/eval.py trees/dev.conll outputs/dev_part1.conll

(nlphw)  ~/local_projects/nlp_hw_dep  python src/eval.py trees/dev.conll outputs/dev_part1.conll
Unlabeled attachment score 83.76
Labeled attachment score 80.49

Finally generate blind test for evaluation:
python src/depModel.py trees/test.conll outputs/test_part1.conll

(nlphw)  ~/local_projects/nlp_hw_dep  python src/depModel.py trees/test.conll outputs/test_part1.conll
[dynet] random seed: 1657944040
[dynet] allocating memory: 512MB
[dynet] memory allocation done.
100...200...300...400...500...600...700...800...900...1000...1100...1200...1300...1400...1500...1600...1700...1800...1900...2000...2100...2200...2300...2400...

Conclusion: By the first part default settings, the unlabeled accuracy is 83.76%, and the labeled accuracy is 80.49%.



Part 2

To change the hidden layers' dimensions, use following:
python src/parser.py --vocabs model/vocabs --model model/model --hidden1 400 --hidden2 400

(nlphw)  ~/local_projects/nlp_hw_dep  python src/parser.py --vocabs model/vocabs --model model/model --hidden1 400 --hidden2 400
[dynet] random seed: 2847457618
[dynet] allocating memory: 512MB
[dynet] memory allocation done.
started epoch 1
current minibatch loss 2.1352994442 progress: 6.96 %
current minibatch loss 1.72770881653 progress: 13.91 %
current minibatch loss 1.32287180424 progress: 20.87 %
current minibatch loss 1.16343533993 progress: 27.82 %
current minibatch loss 0.987162530422 progress: 34.78 %
current minibatch loss 0.894459843636 progress: 41.74 %
current minibatch loss 0.741785943508 progress: 48.69 %
current minibatch loss 0.572202503681 progress: 55.65 %
current minibatch loss 0.530750930309 progress: 62.61 %
current minibatch loss 0.517941176891 progress: 69.56 %
current minibatch loss 0.473667830229 progress: 76.52 %
current minibatch loss 0.386691749096 progress: 83.47 %
current minibatch loss 0.419059067965 progress: 90.43 %
current minibatch loss 0.366789877415 progress: 97.39 %
started epoch 2
current minibatch loss 0.337696611881 progress: 4.87 %
current minibatch loss 0.286913156509 progress: 11.83 %
current minibatch loss 0.326458066702 progress: 18.78 %
current minibatch loss 0.239354714751 progress: 25.74 %
current minibatch loss 0.290056586266 progress: 32.69 %
current minibatch loss 0.252228409052 progress: 39.65 %
current minibatch loss 0.265636503696 progress: 46.61 %
current minibatch loss 0.267701536417 progress: 53.56 %
current minibatch loss 0.273585140705 progress: 60.52 %
current minibatch loss 0.313261151314 progress: 67.47 %
current minibatch loss 0.210434645414 progress: 74.43 %
current minibatch loss 0.236028388143 progress: 81.39 %
current minibatch loss 0.223050430417 progress: 88.34 %
current minibatch loss 0.213416039944 progress: 95.3 %
started epoch 3
current minibatch loss 0.197498068213 progress: 2.78 %
current minibatch loss 0.1922968328 progress: 9.74 %
current minibatch loss 0.230556488037 progress: 16.69 %
current minibatch loss 0.180641666055 progress: 23.65 %
current minibatch loss 0.212652236223 progress: 30.61 %
current minibatch loss 0.211178049445 progress: 37.56 %
current minibatch loss 0.182881176472 progress: 44.52 %
current minibatch loss 0.182732865214 progress: 51.48 %
current minibatch loss 0.178765013814 progress: 58.43 %
current minibatch loss 0.205868512392 progress: 65.39 %
current minibatch loss 0.184397011995 progress: 72.34 %
current minibatch loss 0.217983722687 progress: 79.3 %
current minibatch loss 0.220661848783 progress: 86.26 %
current minibatch loss 0.212861418724 progress: 93.21 %
started epoch 4
current minibatch loss 0.163725674152 progress: 0.7 %
current minibatch loss 0.149197623134 progress: 7.65 %
current minibatch loss 0.145091161132 progress: 14.61 %
current minibatch loss 0.126256555319 progress: 21.56 %
current minibatch loss 0.166253432631 progress: 28.52 %
current minibatch loss 0.141830280423 progress: 35.48 %
current minibatch loss 0.156042829156 progress: 42.43 %
current minibatch loss 0.146538123488 progress: 49.39 %
current minibatch loss 0.138323754072 progress: 56.34 %
current minibatch loss 0.141128540039 progress: 63.3 %
current minibatch loss 0.158895522356 progress: 70.26 %
current minibatch loss 0.143396645784 progress: 77.21 %
current minibatch loss 0.13676430285 progress: 84.17 %
current minibatch loss 0.117247983813 progress: 91.13 %
current minibatch loss 0.127094775438 progress: 98.08 %
started epoch 5
current minibatch loss 0.1018685624 progress: 5.56 %
current minibatch loss 0.0842418074608 progress: 12.52 %
current minibatch loss 0.0987256988883 progress: 19.48 %
current minibatch loss 0.128607839346 progress: 26.43 %
current minibatch loss 0.101854115725 progress: 33.39 %
current minibatch loss 0.0980428829789 progress: 40.35 %
current minibatch loss 0.115682780743 progress: 47.3 %
current minibatch loss 0.0868198871613 progress: 54.26 %
current minibatch loss 0.122511610389 progress: 61.21 %
current minibatch loss 0.115367144346 progress: 68.17 %
current minibatch loss 0.0989502668381 progress: 75.13 %
current minibatch loss 0.131487995386 progress: 82.08 %
current minibatch loss 0.0858892127872 progress: 89.04 %
current minibatch loss 0.120735310018 progress: 95.99 %
started epoch 6
current minibatch loss 0.0908246338367 progress: 3.48 %
current minibatch loss 0.084050051868 progress: 10.43 %
current minibatch loss 0.072455637157 progress: 17.39 %
current minibatch loss 0.0846702605486 progress: 24.35 %
current minibatch loss 0.069044649601 progress: 31.3 %
current minibatch loss 0.0710683315992 progress: 38.26 %
current minibatch loss 0.0715829730034 progress: 45.21 %
current minibatch loss 0.0868093967438 progress: 52.17 %
current minibatch loss 0.104462713003 progress: 59.13 %
current minibatch loss 0.0689030364156 progress: 66.08 %
current minibatch loss 0.068496376276 progress: 73.04 %
current minibatch loss 0.0865624770522 progress: 80.0 %
current minibatch loss 0.0778568387032 progress: 86.95 %
current minibatch loss 0.0940793678164 progress: 93.91 %
started epoch 7
current minibatch loss 0.0711676999927 progress: 1.39 %
current minibatch loss 0.0443689189851 progress: 8.35 %
current minibatch loss 0.0686021149158 progress: 15.3 %
current minibatch loss 0.0602373927832 progress: 22.26 %
current minibatch loss 0.0669117048383 progress: 29.22 %
current minibatch loss 0.0778921097517 progress: 36.17 %
current minibatch loss 0.0585158355534 progress: 43.13 %
current minibatch loss 0.0611958354712 progress: 50.08 %
current minibatch loss 0.062307599932 progress: 57.04 %
current minibatch loss 0.0458355285227 progress: 64.0 %
current minibatch loss 0.0637641772628 progress: 70.95 %
current minibatch loss 0.0436290726066 progress: 77.91 %
current minibatch loss 0.052627954632 progress: 84.86 %
current minibatch loss 0.0586686059833 progress: 91.82 %
current minibatch loss 0.068691663444 progress: 98.78 %

Then execute:
python src/depModel.py trees/dev.conll outputs/dev_part2.conll

(nlphw)  ~/local_projects/nlp_hw_dep  python src/depModel.py trees/dev.conll outputs/dev_part2.conll
[dynet] random seed: 2412806408
[dynet] allocating memory: 512MB
[dynet] memory allocation done.
100...200...300...400...500...600...700...800...900...1000...1100...1200...1300...1400...1500...1600...1700...

To evaluate the part2 result:
python src/eval.py trees/dev.conll outputs/dev_part2.conll

(nlphw)  ~/local_projects/nlp_hw_dep  python src/eval.py trees/dev.conll outputs/dev_part2.conll
Unlabeled attachment score 84.56
Labeled attachment score 81.39

At last, generate blind test reault:
python src/depModel.py trees/test.conll outputs/test_part2.conll

(nlphw)  ~/local_projects/nlp_hw_dep  python src/depModel.py trees/test.conll outputs/test_part2.conll
[dynet] random seed: 266541995
[dynet] allocating memory: 512MB
[dynet] memory allocation done.
100...200...300...400...500...600...700...800...900...1000...1100...1200...1300...1400...1500...1600...1700...1800...1900...2000...2100...2200...2300...2400...

Conclusion: Both unlabeled and labeled accuracies are improved(from 83.76%;80.49% to 84.56%;81.39), which means the higher of the dimensions of hidden layers are good for this model.


Part 3

I tried many combinations, finally, I combined expand hidden layer width and embedding sizes, besides, I added two dropout layers (dropout = 0.3) after the first and the second hiddent layers. Use the following command:
python src/parser.py --vocabs model/vocabs --model model/model --hidden1 600 --hidden2 400 --we 128 --pe 64 --le 64 --epochs 8 --minibatch 500

(nlphw)  ~/local_projects/nlp_hw_dep  python src/parser.py --vocabs model/vocabs --model model/model --hidden1 600 --hidden2 400 --we 128 --pe 64 --le 64 --epochs 8 --minibatch 500
[dynet] random seed: 3459707296
[dynet] allocating memory: 512MB
[dynet] memory allocation done.
started epoch 1
current minibatch loss 2.15958046913 progress: 3.48 %
current minibatch loss 1.5899399519 progress: 6.96 %
current minibatch loss 1.46823561192 progress: 10.43 %
current minibatch loss 1.32399380207 progress: 13.91 %
current minibatch loss 0.98396641016 progress: 17.39 %
current minibatch loss 0.867795050144 progress: 20.87 %
current minibatch loss 0.755992293358 progress: 24.35 %
current minibatch loss 0.65093511343 progress: 27.82 %
current minibatch loss 0.609934747219 progress: 31.3 %
current minibatch loss 0.544559538364 progress: 34.78 %
current minibatch loss 0.483966231346 progress: 38.26 %
current minibatch loss 0.411747604609 progress: 41.74 %
current minibatch loss 0.478364467621 progress: 45.21 %
current minibatch loss 0.472743421793 progress: 48.69 %
current minibatch loss 0.432492792606 progress: 52.17 %
current minibatch loss 0.375443965197 progress: 55.65 %
current minibatch loss 0.34207379818 progress: 59.13 %
current minibatch loss 0.394268959761 progress: 62.61 %
current minibatch loss 0.378211349249 progress: 66.08 %
current minibatch loss 0.413781702518 progress: 69.56 %
current minibatch loss 0.303290486336 progress: 73.04 %
current minibatch loss 0.302948206663 progress: 76.52 %
current minibatch loss 0.40406242013 progress: 80.0 %
current minibatch loss 0.295310825109 progress: 83.47 %
current minibatch loss 0.426348298788 progress: 86.95 %
current minibatch loss 0.345486342907 progress: 90.43 %
current minibatch loss 0.277089506388 progress: 93.91 %
current minibatch loss 0.319022685289 progress: 97.39 %
started epoch 2
current minibatch loss 0.268515259027 progress: 1.04 %
current minibatch loss 0.23617208004 progress: 4.52 %
current minibatch loss 0.231353253126 progress: 8.0 %
current minibatch loss 0.254166036844 progress: 11.48 %
current minibatch loss 0.23674364388 progress: 14.96 %
current minibatch loss 0.294694393873 progress: 18.43 %
current minibatch loss 0.31736177206 progress: 21.91 %
current minibatch loss 0.222719177604 progress: 25.39 %
current minibatch loss 0.201982587576 progress: 28.87 %
current minibatch loss 0.210394874215 progress: 32.35 %
current minibatch loss 0.169520258904 progress: 35.82 %
current minibatch loss 0.196363046765 progress: 39.3 %
current minibatch loss 0.229898110032 progress: 42.78 %
current minibatch loss 0.208862349391 progress: 46.26 %
current minibatch loss 0.197395637631 progress: 49.74 %
current minibatch loss 0.221369117498 progress: 53.21 %
current minibatch loss 0.224110141397 progress: 56.69 %
current minibatch loss 0.250067681074 progress: 60.17 %
current minibatch loss 0.218292161822 progress: 63.65 %
current minibatch loss 0.22323448956 progress: 67.13 %
current minibatch loss 0.227321922779 progress: 70.6 %
current minibatch loss 0.242080956697 progress: 74.08 %
current minibatch loss 0.239092320204 progress: 77.56 %
current minibatch loss 0.230154633522 progress: 81.04 %
current minibatch loss 0.233067557216 progress: 84.52 %
current minibatch loss 0.228094577789 progress: 88.0 %
current minibatch loss 0.183781817555 progress: 91.47 %
current minibatch loss 0.193166300654 progress: 94.95 %
current minibatch loss 0.18107907474 progress: 98.43 %
started epoch 3
current minibatch loss 0.163119494915 progress: 2.09 %
current minibatch loss 0.189934998751 progress: 5.56 %
current minibatch loss 0.155681371689 progress: 9.04 %
current minibatch loss 0.170092180371 progress: 12.52 %
current minibatch loss 0.1588832587 progress: 16.0 %
current minibatch loss 0.175011172891 progress: 19.48 %
current minibatch loss 0.120916463435 progress: 22.96 %
current minibatch loss 0.179131254554 progress: 26.43 %
current minibatch loss 0.135291576385 progress: 29.91 %
current minibatch loss 0.14995470643 progress: 33.39 %
current minibatch loss 0.144820302725 progress: 36.87 %
current minibatch loss 0.127640485764 progress: 40.35 %
current minibatch loss 0.165154546499 progress: 43.82 %
current minibatch loss 0.146083906293 progress: 47.3 %
current minibatch loss 0.209676370025 progress: 50.78 %
current minibatch loss 0.143275842071 progress: 54.26 %
current minibatch loss 0.192748218775 progress: 57.74 %
current minibatch loss 0.176796555519 progress: 61.21 %
current minibatch loss 0.12998971343 progress: 64.69 %
current minibatch loss 0.162935510278 progress: 68.17 %
current minibatch loss 0.145700365305 progress: 71.65 %
current minibatch loss 0.212579578161 progress: 75.13 %
current minibatch loss 0.126972571015 progress: 78.6 %
current minibatch loss 0.21548204124 progress: 82.08 %
current minibatch loss 0.159291297197 progress: 85.56 %
current minibatch loss 0.120512127876 progress: 89.04 %
current minibatch loss 0.20716714859 progress: 92.52 %
current minibatch loss 0.151623740792 progress: 95.99 %
current minibatch loss 0.136029362679 progress: 99.47 %
started epoch 4
current minibatch loss 0.0806546062231 progress: 3.13 %
current minibatch loss 0.131032213569 progress: 6.61 %
current minibatch loss 0.128057986498 progress: 10.09 %
current minibatch loss 0.133285358548 progress: 13.56 %
current minibatch loss 0.129884690046 progress: 17.04 %
current minibatch loss 0.113187134266 progress: 20.52 %
current minibatch loss 0.0751858130097 progress: 24.0 %
current minibatch loss 0.11915487051 progress: 27.48 %
current minibatch loss 0.124873891473 progress: 30.95 %
current minibatch loss 0.111215956509 progress: 34.43 %
current minibatch loss 0.158457383513 progress: 37.91 %
current minibatch loss 0.0967600122094 progress: 41.39 %
current minibatch loss 0.0878790020943 progress: 44.87 %
current minibatch loss 0.109037518501 progress: 48.35 %
current minibatch loss 0.115343719721 progress: 51.82 %
current minibatch loss 0.154144912958 progress: 55.3 %
current minibatch loss 0.109354175627 progress: 58.78 %
current minibatch loss 0.113770537078 progress: 62.26 %
current minibatch loss 0.118188418448 progress: 65.74 %
current minibatch loss 0.143555119634 progress: 69.21 %
current minibatch loss 0.113873451948 progress: 72.69 %
current minibatch loss 0.12095965445 progress: 76.17 %
current minibatch loss 0.124867409468 progress: 79.65 %
current minibatch loss 0.0943143740296 progress: 83.13 %
current minibatch loss 0.0892180651426 progress: 86.6 %
current minibatch loss 0.130715474486 progress: 90.08 %
current minibatch loss 0.132025077939 progress: 93.56 %
current minibatch loss 0.0911820158362 progress: 97.04 %
started epoch 5
current minibatch loss 0.0887812077999 progress: 0.7 %
current minibatch loss 0.0686674565077 progress: 4.17 %
current minibatch loss 0.048517100513 progress: 7.65 %
current minibatch loss 0.0823461860418 progress: 11.13 %
current minibatch loss 0.0880911648273 progress: 14.61 %
current minibatch loss 0.107910908759 progress: 18.09 %
current minibatch loss 0.0740190371871 progress: 21.56 %
current minibatch loss 0.0713627785444 progress: 25.04 %
current minibatch loss 0.0749053731561 progress: 28.52 %
current minibatch loss 0.0580487474799 progress: 32.0 %
current minibatch loss 0.0813063979149 progress: 35.48 %
current minibatch loss 0.122612990439 progress: 38.95 %
current minibatch loss 0.0901858285069 progress: 42.43 %
current minibatch loss 0.0847765877843 progress: 45.91 %
current minibatch loss 0.0895701497793 progress: 49.39 %
current minibatch loss 0.0934124812484 progress: 52.87 %
current minibatch loss 0.0886775031686 progress: 56.34 %
current minibatch loss 0.0817973166704 progress: 59.82 %
current minibatch loss 0.0958120077848 progress: 63.3 %
current minibatch loss 0.0904711037874 progress: 66.78 %
current minibatch loss 0.0758182108402 progress: 70.26 %
current minibatch loss 0.0710975602269 progress: 73.74 %
current minibatch loss 0.108017846942 progress: 77.21 %
current minibatch loss 0.089078091085 progress: 80.69 %
current minibatch loss 0.117810226977 progress: 84.17 %
current minibatch loss 0.0935961902142 progress: 87.65 %
current minibatch loss 0.0916497707367 progress: 91.13 %
current minibatch loss 0.0843036919832 progress: 94.6 %
current minibatch loss 0.0594371445477 progress: 98.08 %
started epoch 6
current minibatch loss 0.0691736340523 progress: 1.74 %
current minibatch loss 0.0467284321785 progress: 5.22 %
current minibatch loss 0.0963418409228 progress: 8.7 %
current minibatch loss 0.0596146658063 progress: 12.17 %
current minibatch loss 0.0509816855192 progress: 15.65 %
current minibatch loss 0.0795949175954 progress: 19.13 %
current minibatch loss 0.0665196180344 progress: 22.61 %
current minibatch loss 0.0627904385328 progress: 26.09 %
current minibatch loss 0.0389446057379 progress: 29.56 %
current minibatch loss 0.044490557164 progress: 33.04 %
current minibatch loss 0.0534526929259 progress: 36.52 %
current minibatch loss 0.0460188798606 progress: 40.0 %
current minibatch loss 0.0702495649457 progress: 43.48 %
current minibatch loss 0.0958001837134 progress: 46.95 %
current minibatch loss 0.0756961330771 progress: 50.43 %
current minibatch loss 0.0584998205304 progress: 53.91 %
current minibatch loss 0.0723519995809 progress: 57.39 %
current minibatch loss 0.0713287293911 progress: 60.87 %
current minibatch loss 0.0376713983715 progress: 64.34 %
current minibatch loss 0.0640237852931 progress: 67.82 %
current minibatch loss 0.0600845292211 progress: 71.3 %
current minibatch loss 0.0830926895142 progress: 74.78 %
current minibatch loss 0.0726536586881 progress: 78.26 %
current minibatch loss 0.0704019367695 progress: 81.73 %
current minibatch loss 0.0516520030797 progress: 85.21 %
current minibatch loss 0.0736167505383 progress: 88.69 %
current minibatch loss 0.0698063671589 progress: 92.17 %
current minibatch loss 0.0669381394982 progress: 95.65 %
current minibatch loss 0.0911008864641 progress: 99.12 %
started epoch 7
current minibatch loss 0.028910363093 progress: 2.78 %
current minibatch loss 0.0483077950776 progress: 6.26 %
current minibatch loss 0.0312701798975 progress: 9.74 %
current minibatch loss 0.0350267216563 progress: 13.22 %
current minibatch loss 0.0301962308586 progress: 16.69 %
current minibatch loss 0.0567638166249 progress: 20.17 %
current minibatch loss 0.0494046285748 progress: 23.65 %
current minibatch loss 0.0317923650146 progress: 27.13 %
current minibatch loss 0.0415329858661 progress: 30.61 %
current minibatch loss 0.0222932118922 progress: 34.09 %
current minibatch loss 0.0399365611374 progress: 37.56 %
current minibatch loss 0.0277296062559 progress: 41.04 %
current minibatch loss 0.0616777464747 progress: 44.52 %
current minibatch loss 0.0627894103527 progress: 48.0 %
current minibatch loss 0.0681844949722 progress: 51.48 %
current minibatch loss 0.0283432994038 progress: 54.95 %
current minibatch loss 0.0303088929504 progress: 58.43 %
current minibatch loss 0.0332611724734 progress: 61.91 %
current minibatch loss 0.0392867587507 progress: 65.39 %
current minibatch loss 0.0480807200074 progress: 68.87 %
current minibatch loss 0.0391196087003 progress: 72.34 %
current minibatch loss 0.0823011472821 progress: 75.82 %
current minibatch loss 0.0519366450608 progress: 79.3 %
current minibatch loss 0.0634905099869 progress: 82.78 %
current minibatch loss 0.0363423749804 progress: 86.26 %
current minibatch loss 0.0451958626509 progress: 89.73 %
current minibatch loss 0.032574057579 progress: 93.21 %
current minibatch loss 0.0377295166254 progress: 96.69 %
started epoch 8
current minibatch loss 0.0199957620353 progress: 0.35 %
current minibatch loss 0.0338201709092 progress: 3.83 %
current minibatch loss 0.0373535566032 progress: 7.3 %
current minibatch loss 0.0348003692925 progress: 10.78 %
current minibatch loss 0.015655901283 progress: 14.26 %
current minibatch loss 0.0296956263483 progress: 17.74 %
current minibatch loss 0.0350355096161 progress: 21.22 %
current minibatch loss 0.0263719689101 progress: 24.69 %
current minibatch loss 0.0322130843997 progress: 28.17 %
current minibatch loss 0.0266995318234 progress: 31.65 %
current minibatch loss 0.0617687813938 progress: 35.13 %
current minibatch loss 0.0238824095577 progress: 38.61 %
current minibatch loss 0.0443770326674 progress: 42.08 %
current minibatch loss 0.035393550992 progress: 45.56 %
current minibatch loss 0.0500323809683 progress: 49.04 %
current minibatch loss 0.0630233511329 progress: 52.52 %
current minibatch loss 0.0457957386971 progress: 56.0 %
current minibatch loss 0.0521074794233 progress: 59.47 %
current minibatch loss 0.0316213220358 progress: 62.95 %
current minibatch loss 0.0316677317023 progress: 66.43 %
current minibatch loss 0.026955684647 progress: 69.91 %
current minibatch loss 0.0654135420918 progress: 73.39 %
current minibatch loss 0.0314002931118 progress: 76.87 %
current minibatch loss 0.0505280792713 progress: 80.34 %
current minibatch loss 0.0345000140369 progress: 83.82 %
current minibatch loss 0.0307902209461 progress: 87.3 %
current minibatch loss 0.0384001545608 progress: 90.78 %
current minibatch loss 0.0365776158869 progress: 94.26 %
current minibatch loss 0.0504313595593 progress: 97.73 %

develop "dev_part3.coll":
python src/depModel.py trees/dev.conll outputs/dev_part3.conll

(nlphw)  ~/local_projects/nlp_hw_dep  python src/depModel.py trees/dev.conll outputs/dev_part3.conll
[dynet] random seed: 996786930
[dynet] allocating memory: 512MB
[dynet] memory allocation done.
100...200...300...400...500...600...700...800...900...1000...1100...1200...1300...1400...1500...1600...1700...

Evaluate:
python src/eval.py trees/dev.conll outputs/dev_part3.conll

(nlphw)  ~/local_projects/nlp_hw_dep  python src/eval.py trees/dev.conll outputs/dev_part3.conll
Unlabeled attachment score 84.35
Labeled attachment score 81.02

Build blind test result "test_part3.conll":
python src/depModel.py trees/test.conll outputs/test_part3.conll

Conclusion: Comparing to part 2, simply increasing the width of the hidden layer will start to decrease the accuracy. However, when increasing the hidden layer width, adding the dropout layer (dropout = 0.3) for regularization would help to increase the accuracy. I also changed the size of embedding sizes, which didn't affect the result much. Then I tried to changed the minibatch size, which randomly affected the accuracy. But overall accuracy won't change to much by these changes. For this submitted test, the accuracy scores for unlabeled and labeled are 84.35% and 81.02%.